# Databricks notebook source
# MAGIC %md
# MAGIC # Runbook Generator Notebook
# MAGIC Generates the final runbook markdown using a template-based approach (reliable for free tier).

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import os
import pandas as pd
import datetime
import uuid

# COMMAND ----------

dbutils.widgets.text("output_path", "/dbfs/tmp/ps_ai_runbook_gen/runbooks")
dbutils.widgets.text("model_type", "distilbert-base-uncased")
output_path = dbutils.widgets.get("output_path")
model_type = dbutils.widgets.get("model_type")

# COMMAND ----------

# Load Gold Data
try:
    df = spark.table("gold_engagement_vectors")
    docs = df.collect()
    print(f"Loaded {len(docs)} documents for generation")
except Exception as e:
    print(f"Error loading gold table: {e}")
    dbutils.notebook.exit("FAILED: Could not load gold table")

# COMMAND ----------

# Extract aggregated insights
all_entities = []
tech_stack = set()
risks = []
dates = []

for row in docs:
    if row['entities']:
        for entity in row['entities']:
            all_entities.append(entity)
            if "TECH:" in entity:
                tech_stack.add(entity.replace("TECH: ", ""))
            if "DATE:" in entity:
                dates.append(entity.replace("DATE: ", ""))

# Simple "AI" Logic for Executive Summary
doc_count = len(docs)
tech_summary = ", ".join(list(tech_stack)[:5]) if tech_stack else "Standard Databricks Stack"

executive_summary = f"""
This engagement involves the analysis of {doc_count} key documents. 
The primary technology stack identified includes **{tech_summary}**. 
The engagement is currently in the planning/execution phase with key milestones identified in the attached schedule.
"""

# COMMAND ----------

# Generate Professional Runbook Markdown
markdown_output = f"""# üìò Professional Services Engagement Runbook
**Generated by:** PS AI Runbook Generator ({model_type})  
**Date:** {datetime.datetime.now().strftime("%Y-%m-%d %H:%M")}  
**Status:** DRAFT

---

## 1. Executive Summary
{executive_summary}

## 2. Engagement Scope
The following documents were ingested and analyzed to create this runbook:
"""

for row in docs:
    filename = row['path'].split('/')[-1]
    markdown_output += f"- üìÑ **{filename}**\n"

markdown_output += """
## 3. Technical Architecture
Based on the analysis, the following core components are involved:
"""

if tech_stack:
    for tech in tech_stack:
        markdown_output += f"- ‚úÖ {tech}\n"
else:
    markdown_output += "- No specific technologies detected in current document set.\n"

markdown_output += """
## 4. Identified Risks & Assumptions
- **Risk:** Data quality dependencies for ingested sources.
- **Risk:** Timeline constraints based on identified dates.
- **Assumption:** All provided documents are current and approved.

## 5. Next Steps
1. Review and validate the extracted technical stack.
2. Confirm key dates found in documents:
"""

if dates:
    for date in dates:
        markdown_output += f"   - üìÖ {date}\n"
else:
    markdown_output += "   - No specific dates extracted.\n"

markdown_output += """
---
*Generated via Databricks PS AI Tooling Prototype*
"""

# COMMAND ----------

# Write to DBFS
try:
    # Get Run ID from context if available, else generate one
    try:
        context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()
        run_id = str(context.jobId().get()) if context.jobId().isDefined() else str(uuid.uuid4())
    except:
        run_id = str(uuid.uuid4())
        
    print(f"Writing runbook for Run ID: {run_id}")

    # Create directory
    # Handle DBFS path correctly
    if output_path.startswith("dbfs:"):
        local_output_path = output_path.replace("dbfs:", "/dbfs")
    else:
        local_output_path = output_path
        
    final_dir = os.path.join(local_output_path, run_id)
    os.makedirs(final_dir, exist_ok=True)
    
    file_path = os.path.join(final_dir, "runbook.md")
    with open(file_path, "w") as f:
        f.write(markdown_output)
    
    print(f"‚úÖ Runbook successfully written to {file_path}")
    
    # IMPORTANT: Return the runbook content as notebook output
    # This allows retrieval via Jobs API even without DBFS access (Community Edition compatible)
    dbutils.notebook.exit(markdown_output)
    
except Exception as e:
    print(f"‚ùå Error writing runbook: {e}")
    dbutils.notebook.exit(f"FAILED: Could not write runbook - {str(e)}")
